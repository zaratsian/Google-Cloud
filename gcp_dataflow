

################################################################################################################
#
#   Google Cloud Dataflow
#
#   Usage: 
'''
python gcp_dataflow.py \
    --runner DirectRunner \
    --job_name 'dzdataflowjob1' \
    --batch_size 10 \
    --project_id dzproject20180301 \
    --input_topic projects/dzproject20180301/topics/chicagotraffic \
    --dataset_name chicago_traffic \
    --table_name traffic_segments1 \
    --table_schema '_direction:STRING,_fromst:STRING,_last_updt:STRING,_length:FLOAT,_lif_lat:FLOAT,_lit_lat:FLOAT,_lit_lon:FLOAT,_strheading:STRING,_tost:STRING,_traffic:STRING,segmentid:STRING,start_lon:FLOAT,street:STRING'
'''
#   pip install google-cloud-dataflow
#
################################################################################################################


from __future__ import absolute_import
import logging
import argparse
import apache_beam as beam
import apache_beam.transforms.window as window
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.options.pipeline_options import StandardOptions
from apache_beam.options.pipeline_options import SetupOptions
from apache_beam.io import ReadFromText
from apache_beam.io import WriteToText


################################################################################################################
#
#   Functions
#
################################################################################################################

def parse_pubsub(line):
    import json
    record = json.loads(line)
    #record = json.loads(json.dumps({"direction":record['_direction'], "street":record['street']}))    #'direction:STRING,state:STRING'
    #return (record['mac']), (record['status']), (record['datetime'])
    return record


def run(argv=None):
    """Build and run the pipeline."""
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--runner',       required=False, default='DirectRunner',       help='Dataflow Runner - DataflowRunner or DirectRunner (local)')
    parser.add_argument('--job_name',     required=False, default='dzdataflowjob1',     help='Dataflow Job Name')
    parser.add_argument('--batch_size',   required=False, default='10',                 help='Dataflow Batch Size')
    parser.add_argument('--project_id',   required=False, default='dzproject20180301',  help='GCP Project ID')
    parser.add_argument('--input_topic',  required=False, default='',                   help='Input PubSub Topic: projects/<project_id>/topics/<topic_name>')
    parser.add_argument('--dataset_name', required=False, default='chicago_traffic',    help='Output BigQuery Dataset') 
    parser.add_argument('--table_name',   required=False, default='',                   help='Output BigQuery Table')
    parser.add_argument('--table_schema', required=False, default='',                   help='Output BigQuery Schema')
    
    known_args, pipeline_args = parser.parse_known_args(argv)
    
    pipeline_args.extend([
          # CHANGE 2/5: (OPTIONAL) Change this to DataflowRunner to
          # run your pipeline on the Google Cloud Dataflow Service. DirectRunner (local)
          #'--runner=DirectRunner',
          '--runner=' + str(known_args.runner),
          # CHANGE 3/5: Your project ID is required in order to run your pipeline on
          # the Google Cloud Dataflow Service.
          #'--project=dzproject20180301',
          '--project=' + str(known_args.project_id),
          # CHANGE 4/5: Your Google Cloud Storage path is required for staging local
          # files.
          '--staging_location=gs://tmp_dataflow/staging',
          # CHANGE 5/5: Your Google Cloud Storage path is required for temporary
          # files.
          '--temp_location=gs://tmp_dataflow/tmp',
          #'--job_name=dzdataflowjob1',
          '--job_name=' + str(known_args.job_name),
      ])
    
    pipeline_options = PipelineOptions(pipeline_args)
    #pipeline_options.view_as(SetupOptions).save_main_session = True
    pipeline_options.view_as(StandardOptions).streaming = True
    
    with beam.Pipeline(options=pipeline_options) as p:
        
        # Read the pubsub topic into a PCollection.
        events = ( p | beam.io.ReadStringsFromPubSub(known_args.input_topic) )
        
        # Tranform events
        transformed = (events 
                    | beam.Map(parse_pubsub))
        
        # Persist to BigQuery
        transformed | 'Write' >> beam.io.WriteToBigQuery(
                        table=known_args.table_name,
                        dataset=known_args.dataset_name,
                        project=known_args.project_id,
                        schema=known_args.table_schema,
                        create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
                        write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,
                        batch_size=int(known_args.batch_size))


################################################################################################################
#
#   Main
#
################################################################################################################

if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    run()


################################################################################################################
#
#   References
#
################################################################################################################

'''
https://cloud.google.com/dataflow/docs/quickstarts

https://beam.apache.org/documentation/sdks/pydoc/2.4.0/
https://beam.apache.org/documentation/sdks/pydoc/2.4.0/apache_beam.io.gcp.bigquery.html#apache_beam.io.gcp.bigquery.WriteToBigQuery

https://github.com/apache/beam/tree/master/sdks/python/apache_beam/examples
'''


'''
NOTE: BigQuery expects JSON newline seperated data, such as
{"name":"dan","age"34}
{"name":"frank","age"54}
{"name":"dean","age"64}

NOTE: Table Schema (for BigQuery)
  'var01:STRING',
  'var02:BYTES',
  'var03:INTEGER',
  'var04:FLOAT',
  'var05:BOOLEAN',
  'var06:DATE',
  'var07:TIME',
  'var08:TIMESTAMP',
  'var09:DATETIME',
  'var10:RECORD',
'''

#ZEND
